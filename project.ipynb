{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tetsumichi Umada (Telly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "The project idea originally comes from [DeepMoji](https://deepmoji.mit.edu), which translate given text messages into Emojis to describe their emotions, using millions of text messages and state-of-art algorithms. For this project, we predict emotions associated texts. Nowadays, people often send a text to each other. However, it is hard for people to understand and know a sender's emotions from the text. The classifier will further help to represent emotions associated with the text messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets**: The data comes from [DeepMoji GitHub repository](https://github.com/bfelbo/DeepMoji/tree/master/data). All the data is in `pickle` format. Using python2 script, we unpickle the data and save it as a`txt` file. Then, we use python3 to apply machine learning algorithms. \n",
    "\n",
    "<br>\n",
    "The python script to extract `pickle` data is saved as `script.py`. The file name needs to be specified at the top of the script. Then, run the program with `python script.py > <filen_name>.txt` to save into a new txt file. And, each line contains text and its label, formatted with: `[<label>] <text>`. \n",
    "\n",
    "<br>\n",
    "We use `psychExp` datasets. Each instance is associated with seven kinds of emotions: joy, fear, anger, sadness, disgust, shame, guilt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hala madrid\n"
    }
   ],
   "source": [
    "print('hala madrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name): \n",
    "    data_list  = []\n",
    "    with open(file_name, 'r') as f: \n",
    "        for line in f: \n",
    "            line = line.strip() \n",
    "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
    "            text = line[line.find(\"]\")+1:].strip()\n",
    "            data_list.append([label, text])\n",
    "    return data_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"olympic.txt\"\n",
    "file_name = \"data/psychExp.txt\"\n",
    "psychExp_txt = read_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The number of instances: 7480\n"
    }
   ],
   "source": [
    "print(\"The number of instances: {}\".format(len(psychExp_txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data example: \n['1. 0. 0. 0. 0. 0. 0.', 'During the period of falling in love, each time that we met and especially when we had not met for a long time.']\n"
    }
   ],
   "source": [
    "print(\"Data example: \")\n",
    "print(psychExp_txt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features       \n",
    "From the given texts, we made ngrams with a range from 1 to 4. The feature also includes the any punctuations such as `!`, `?`, or `.`. If the data originally come form the Tweets, it contains hash tags (such as `#olympics2012` or `#olympiics`). They are kept as hashtags. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(token, n): \n",
    "    output = []\n",
    "    for i in range(n-1, len(token)): \n",
    "        ngram = ' '.join(token[i-n+1:i+1])\n",
    "        output.append(ngram) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(text, nrange=(1, 1)):\n",
    "    text_features = [] \n",
    "    text = text.lower() \n",
    "\n",
    "    # 1. treat alphanumeric characters as word tokens\n",
    "    # Since tweets contain #, we keep it as a feature\n",
    "    # Then, extract all ngram lengths\n",
    "    text_alphanum = re.sub('[^a-z0-9#]', ' ', text)\n",
    "    for n in range(nrange[0], nrange[1]+1): \n",
    "        text_features += ngram(text_alphanum.split(), n)\n",
    "    \n",
    "    # 2. treat punctuations as word token\n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    text_features += ngram(text_punc.split(), 1)\n",
    "    \n",
    "    # 3. Return a dictinaory whose keys are the list of elements \n",
    "    # and their values are the number of times appearede in the list.\n",
    "    return Counter(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Counter({'i': 1, 'love': 1, 'you': 1, '!': 1})\nCounter({'aly': 1, 'wins': 1, 'the': 1, 'gold': 1, '#olympics': 1, '!!!!!!': 1, '#': 1})\nCounter({'aly': 1, 'wins': 1, 'the': 1, 'gold': 1, '#olympics': 1, 'aly wins': 1, 'wins the': 1, 'the gold': 1, 'gold #olympics': 1, '!!!!!!': 1, '#': 1})\n"
    }
   ],
   "source": [
    "print(create_feature(\"I love you!\"))\n",
    "print(create_feature(\" aly wins the gold!!!!!!  #olympics\"))\n",
    "print(create_feature(\" aly wins the gold!!!!!!  #olympics\", (1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cnvert the lables and Add features to text data**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(item, name): \n",
    "    items = list(map(float, item.split()))\n",
    "    label = \"\"\n",
    "    for idx in range(len(items)): \n",
    "        if items[idx] == 1: \n",
    "            label += name[idx] + \" \"\n",
    "    \n",
    "    return label.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "\n",
    "X_all = []\n",
    "y_all = []\n",
    "for label, text in psychExp_txt:\n",
    "    y_all.append(convert_label(label, emotions))\n",
    "    X_all.append(create_feature(text, nrange=(1, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features example: \n",
      "Counter({'time': 2, 'we': 2, 'met': 2, 'during': 1, 'the': 1, 'period': 1, 'of': 1, 'falling': 1, 'in': 1, 'love': 1, 'each': 1, 'that': 1, 'and': 1, 'especially': 1, 'when': 1, 'had': 1, 'not': 1, 'for': 1, 'a': 1, 'long': 1, 'during the': 1, 'the period': 1, 'period of': 1, 'of falling': 1, 'falling in': 1, 'in love': 1, 'love each': 1, 'each time': 1, 'time that': 1, 'that we': 1, 'we met': 1, 'met and': 1, 'and especially': 1, 'especially when': 1, 'when we': 1, 'we had': 1, 'had not': 1, 'not met': 1, 'met for': 1, 'for a': 1, 'a long': 1, 'long time': 1, 'during the period': 1, 'the period of': 1, 'period of falling': 1, 'of falling in': 1, 'falling in love': 1, 'in love each': 1, 'love each time': 1, 'each time that': 1, 'time that we': 1, 'that we met': 1, 'we met and': 1, 'met and especially': 1, 'and especially when': 1, 'especially when we': 1, 'when we had': 1, 'we had not': 1, 'had not met': 1, 'not met for': 1, 'met for a': 1, 'for a long': 1, 'a long time': 1, 'during the period of': 1, 'the period of falling': 1, 'period of falling in': 1, 'of falling in love': 1, 'falling in love each': 1, 'in love each time': 1, 'love each time that': 1, 'each time that we': 1, 'time that we met': 1, 'that we met and': 1, 'we met and especially': 1, 'met and especially when': 1, 'and especially when we': 1, 'especially when we had': 1, 'when we had not': 1, 'we had not met': 1, 'had not met for': 1, 'not met for a': 1, 'met for a long': 1, 'for a long time': 1, ',': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"features example: \")\n",
    "print(X_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Label example:\njoy\n"
    }
   ],
   "source": [
    "print(\"Label example:\")\n",
    "print(y_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers \n",
    "The following classifiers are used for this project:\n",
    "+ `SVC`\n",
    "+ `LinearSVC`\n",
    "+ `RandomForestClassifier`\n",
    "+ `DecisionTreeClassifier`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data split**: The data is split into 80-20 ratio where 80% for the training and 20% for the testing. We keep 20% to make sure that we have enough amount of the data for testing. We used `train_test_split` to split the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_all, y_all, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_test(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    \n",
    "#     print(\"Training acc: {}\".format(train_acc))\n",
    "#     print(\"Test acc    : {}\".format(test_acc))\n",
    "    \n",
    "    return train_acc, test_acc    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DictVectorizer**: The above converts the labeled features into sparse vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer(sparse = True)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "| Classifier                | Training Accuracy | Test Accuracy |\n| ------------------------- | ----------------- | ------------- |\nClifs size\n[SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False), LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n          multi_class='ovr', penalty='l2', random_state=123, tol=0.0001,\n          verbose=0), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=123,\n                       verbose=0, warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=None, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='best')]\n| SVC                       |         0.9067513 |     0.4512032 |\n| LinearSVC                 |         0.9988302 |     0.5768717 |\n| RandomForestClassifier    |         0.9988302 |     0.5541444 |\n| DecisionTreeClassifier    |         0.9988302 |     0.4612299 |\n"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Classifiers \n",
    "svc = SVC()\n",
    "lsvc = LinearSVC(random_state=123)\n",
    "rforest = RandomForestClassifier(random_state=123)\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "clifs = [svc, lsvc, rforest, dtree]\n",
    "\n",
    "# train and test them \n",
    "print(\"| {:25} | {} | {} |\".format(\"Classifier\", \"Training Accuracy\", \"Test Accuracy\"))\n",
    "print(\"| {} | {} | {} |\".format(\"-\"*25, \"-\"*17, \"-\"*13))\n",
    "print(\"Clifs size\")\n",
    "print(clifs)\n",
    "for clf in clifs: \n",
    "    clf_name = clf.__class__.__name__\n",
    "    train_acc, test_acc = train_test(clf, X_train, X_test, y_train, y_test)\n",
    "    print(\"| {:25} | {:17.7f} | {:13.7f} |\".format(clf_name, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**: The training and test accuracies for each classifier are summarized in the table below.\n",
    "\n",
    "\n",
    "| Classifier                | Training Accuracy | Test Accuracy |\n",
    "| ------------------------- | ----------------- | ------------- |\n",
    "| SVC                       |         0.1458890 |     0.1410428 |\n",
    "| LinearSVC                 |         0.9988302 |     0.5768717 |\n",
    "| RandomForestClassifier    |         0.9911430 |     0.4304813 |\n",
    "| DecisionTreeClassifier    |         0.9988302 |     0.4585561 |\n",
    "\n",
    "\n",
    "\n",
    "By looking at the numbers, the support vector machine works good, meaning it's not overfitting and underfitting. The test accuracy is slightly higher than the other classifiers. The other classifiers are overfitting because the training accuracy is very higher than the test accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Tuning**: Since `LinearSVC` seems to be working good, we further try to tune parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1\n2\n3\n4\nValidation acc: 0.5608291911495573\nTraining acc: 0.9988302139037433\nTest acc    : 0.5768716577540107\nBest parameter: {'C': 1, 'tol': 0.01}\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C':[1, 2, 3, 5, 10, 15, 20, 30, 50, 70, 100], \n",
    "             'tol':[0.1, 0.01, 0.001, 0.0001, 0.00001]}\n",
    "print(\"1\")\n",
    "lsvc = LinearSVC(random_state=123)\n",
    "print(\"2\")\n",
    "grid_obj = GridSearchCV(lsvc, param_grid = parameters, cv=5)\n",
    "print(\"3\")\n",
    "grid_obj.fit(X_train, y_train)\n",
    "print(\"4\")\n",
    "\n",
    "print(\"Validation acc: {}\".format(grid_obj.best_score_))\n",
    "print(\"Training acc: {}\".format(accuracy_score(y_train, grid_obj.predict(X_train))))\n",
    "print(\"Test acc    : {}\".format(accuracy_score(y_test, grid_obj.predict(X_test))))\n",
    "print(\"Best parameter: {}\".format(grid_obj.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final model**: The final model for this project performs test accuracy 0.576872. Although we tried to tune to minimize the overfitting, it seems to be not working well. As we expected, the accuracy is low.... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[112  27  16  23  12  19  19]\n [ 20 113  11  11  12  17   9]\n [  9  18 157   3  11  16  10]\n [ 19  12  15 107  12  17  36]\n [  7   9   7   7 155  18   8]\n [ 17   9  17  12  21 137  10]\n [ 31  19  10  30  17  10  82]]\n"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_test, grid_obj.predict(X_test))\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a21172815ccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook-6.0.3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2315\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-108>\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook-6.0.3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook-6.0.3/lib/python3.8/site-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook-6.0.3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3405\u001b[0m         \"\"\"\n\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3407\u001b[0;31m         \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/notebook-6.0.3/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[0;34m(gui, gui_select)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \"\"\"\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgui\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "l = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "l.sort()\n",
    "df_cm = pd.DataFrame(matrix, index = l, columns = l)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "plt.show()\n",
    "\n",
    "# Reference https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "joy       (1. 0. 0. 0. 0. 0. 0.)  1084\nanger     (0. 0. 1. 0. 0. 0. 0.)  1080\nsadness   (0. 0. 0. 1. 0. 0. 0.)  1079\nfear      (0. 1. 0. 0. 0. 0. 0.)  1078\ndisgust   (0. 0. 0. 0. 1. 0. 0.)  1057\nguilt     (0. 0. 0. 0. 0. 0. 1.)  1057\nshame     (0. 0. 0. 0. 0. 1. 0.)  1045\n"
    }
   ],
   "source": [
    "# Get counts for each label\n",
    "label_freq = {}\n",
    "for label, _ in psychExp_txt: \n",
    "    label_freq[label] = label_freq.get(label, 0) + 1\n",
    "\n",
    "# print the labels and their counts in sorted order \n",
    "for l in sorted(label_freq, key=label_freq.get, reverse=True):\n",
    "    print(\"{:10}({})  {}\".format(convert_label(l, emotions), l, label_freq[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the distributions of the labels, the data is distributed well. The errors probably happen because there are not enough data to train the classifier. Also, ngram with the range of 1 to 4 might not be enough or adding noise to the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work \n",
    "\n",
    "To be able to accurately analyze the text, we probably need to have more data to train the classifiers. Also, it would be better to further experiment with features. (I'm not sure what we can use, but probably there is something to make features) At the same time, by adding more data with emoji, we can predict more emoji associated with the texts. \n",
    "\n",
    "<br> \n",
    "For the future work, we want to have more data and more emoji to analyze and represent the sentiment of the texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {\"joy\":\"ðŸ˜‚\", \"fear\":\"ðŸ˜±\", \"anger\":\"ðŸ˜ \", \"sadness\":\"ðŸ˜¢\", \"disgust\":\"ðŸ˜’\", \"shame\":\"ðŸ˜³\", \"guilt\":\"ðŸ˜³\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "shame i don't like her\nsadness I don't like it\nfear My car skidded on the wet street\nsadness My cat died\n"
    }
   ],
   "source": [
    "t1 = \"i don't like her\"\n",
    "t2 = \"I don't like it\"\n",
    "t3 = \"My car skidded on the wet street\"\n",
    "t4 = \"My cat died\"\n",
    "\n",
    "texts = [t1, t2, t3, t4]\n",
    "for text in texts: \n",
    "    features = create_feature(text, nrange=(1, 4))\n",
    "    features = vectorizer.transform(features)\n",
    "    prediction = grid_obj.predict(features)[0]\n",
    "    print(\"{} {}\".format(prediction, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference \n",
    "+ [DeepMoji](https://www.media.mit.edu/projects/deepmoji/overview/)\n",
    "+ [DeepMoji GitHub](https://github.com/bfelbo/DeepMoji)\n",
    "+ [Multiclass and multilabel algorithms](http://scikit-learn.org/stable/modules/multiclass.html)\n",
    "+ [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "+ [sklearn.svm.LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    "+ [sklearn.ensemble.RandomForestClassifie](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "+ [sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}